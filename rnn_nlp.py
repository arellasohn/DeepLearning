# -*- coding: utf-8 -*-
"""RNN_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xDEHPeuoTTlSHQIKeou7GyhXrNqOhLH7
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Embedding,LSTM,Bidirectional

import json
import urllib
import numpy as np

url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'
urllib.request.urlretrieve(url,'sarcasm.json')

with open('sarcasm.json') as f:
    datas = json.load(f)

"""- sarcasm.json
> * article_link: 기사의 link
> * headline: 기사의 제목 (X)
> * is_sarcastic: 비꼬는 기사인지의 여부 (y)
"""

sentences = [] # headline
labels = []    # is_sarcastic

for data in datas:
  sentences.append(data['headline'])
  labels.append(data['is_sarcastic'])

print(sentences[:2]) # X
print(labels[:2])    # y

# 20000개의 문장은 training data로, 나머지 문장은 test data로 사용함
train_sentences = sentences[:20000]
train_labels = labels[:20000]

test_sentences = sentences[20000:]
test_labels = labels[20000:]
# training과 test를 위한 text와 label

"""- text를 수로 변경하는 과정이 필요

# Tokenizer
"""

tok = Tokenizer(num_words=5000,oov_token='<OOV>')
# Tokenizer(num_words, oov_token)
# num_words: 사전에 포함된 단어의 수
# oov_token: 단어 사전에 포함되지 않는 단어를 대체할 문자열

# Tokenizer에 text를 입력하여 사전을 생성: 단어에 숫자를 배정
tok.fit_on_texts(train_sentences)

tok.word_index
# 단어의 숫자가 할당되는 규칙의 등장하는 빈도수이다. (많이 등장할 수 록 작은 값을 갖는다.)

# text인 sentence를 숫자 sequence로 변경
train_seq = tok.texts_to_sequences(train_sentences)
test_seq = tok.texts_to_sequences(test_sentences)

print(train_sentences[100])
print(train_seq[100])

# 문장 길이 통일
train_pad = pad_sequences(train_seq,maxlen=15,truncating='post',padding='post')
# pad_sequences(sequences,maxlen,truncating,padding)
# - sequences: text에서 number로 변경한 sequences
# - maxlen: 문장의 길이 (모든 문장의 길이를 동일하게 조정해야 함)
# - truncating: 지정된 길이보다 긴 문장을 잘라낼 위치 (post:뒤, pre:앞)
# - padding: 지정된 길이보다 짧은 문장에 0을 채울 위치 (post:뒤, pre:앞)
test_pad = pad_sequences(test_seq,maxlen=15,truncating='post',padding='post')

# pad_sequences의 결과는 ndarray이다.
# train_sentences와 train_seq은 list type이다.

# labels(y)도 ndarray로 변경함
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

"""# model"""

model = Sequential([
    Embedding(5000,16,input_length=15),
    # Embedding(사전에 포함된 단어의 수, 줄이고자하는 차원, input_length=문장의 길이)
    Bidirectional(LSTM(64,return_sequences=True)),
    # LSTM(node의 수, return_sequences)
    # 다음 layer가 LSTM으로 연결된 경우, return_sequence를 True로 설정한다. default로 False가 설정되어 있음
    Bidirectional(LSTM(32)),

    Dense(32,activation='relu'),
    # 0 또는 1, 2개의 class로 분류하므로 output layer의 node는 2개로 지정함
    Dense(2,activation='softmax')
])

model.summary()

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])

result = model.fit(train_pad,train_labels,epochs=20,batch_size=256,validation_split=0.3)

model.evaluate(test_pad,test_labels)

import matplotlib.pyplot as plt

plt.plot(result.history['loss'],label='training')
plt.plot(result.history['val_loss'],label='test')
plt.legend()

import matplotlib.pyplot as plt

plt.plot(result.history['acc'],label='training')
plt.plot(result.history['val_acc'],label='test')
plt.legend()